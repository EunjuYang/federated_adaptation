data_type: text
repo_path: ./
save_name: text_av_fbr

# Training params
test_batch_size: 10
lr: 1
momentum: 0
decay: 0
batch_size: 20
only_eval: false
scratch: false
partial_test: 10
multi_gpu: false

freeze_base: false

ewc: false
lamb: 5000

kd: false
alpha: 0.95
temperature: 6

# FedLearning params, 80000
no_models: 79999
total_rounds: 5000
retrain_no_times: 100
number_of_total_participants: 80000
eta: 1

word_dictionary_path: /home/ty367/federated/utils/50k_word_dictionary.pt
local_test_perc: 10

recreate_dataset: false
resumed_model: model_text_Sep.29_15.54.52/model_last.pt.tar.best
### averaging: model_text_Sep.29_15.54.52/model_last.pt.tar.best
### averaging diff: model_text_Oct.15_15.55.39/model_last.pt.tar.best
### median: model_text_Nov.13_02.48.35/model_last.pt.tar.best
resumed_fisher: /home/ty367/federated/data/averaing_fisher.pt
### averaging: averaing_fisher.pt
### averaging diff: averaing_diff_fisher.pt
### median: median_fisher.pt

scale_weights: 100

# configs for the NLP model
emsize: 200
nhid: 200
nlayers: 2
dropout: 0.2
tied: true
bptt: 64
clip: 0.25
seed: 1
data_folder: /home/ty367/federated/data